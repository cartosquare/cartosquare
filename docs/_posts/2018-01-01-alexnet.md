---
title: AlexNet paper reading
date: 2018-01-01 21:08:05
tags:
---


ImageNet LSVRC-2010、2012冠军网络。由5个卷积层（其中一些卷积层后面是max-pooling层），3个直连层，以及最后一个输出为1000维向量的softmax层组成。为了防止过拟合，AlexNet在直连层的后面添加了dropout层。

## 图片预处理

不同大小的图片缩放到短边为256，然后从中心crop出256*256的图片，图片的像素值减去整个数据集的平均值。

## 网络架构
### 非线性的ReLU激活函数
过去标准的激活函数是tanh函数或是sigmoid等非线性函数，AlexNet网络使用了ReLu这种非线性函数，好处是不会发散，训练时间更快。

### Local Response Normalization
Local Normalization 增大网络的泛化能力。记$a\_{x,y}^{i}$是使用卷积核i在位置$x,y$上的神经元的ReLU激活输出，那么response-normalize输出$b\_{x,y}^{i}$是：
$$
b\_{x,y}^{i} = a\_{x,y}^{i} \; / \; (k + \alpha \sum\_{j=max(0,i-n/2)}^{min(N-1,i+n/2)} (a\_{x,y}^{i})^2)^{\beta}
$$
上式中的求和的是同一个位置的临近kernal的神经元，其中N是这个层总共的kernal个数。 这个求和相当于是把临近的距离不超过N/2的神经元累加在一起。其中的$k,n,\alpha,\beta$都是超参数，在AlexNet里设置成了$k=2,n=5,\alpha = 10^{-4}, \beta = 0.75$。
这个正规化层被应用到了某些ReLU层之后。AlexNet的实验表明，在ImageNet数据集上，使用和不使用LRN层会有1-2个点的差异。

### Overlapping Pooling
AlexNet网络表明使用Overlapping Pooling可以提升0.3-0.4个点，并且带Overlapping Pooling的网络更不容易过拟合。

### 总体结构


## Reducing Overfitting
### Data Argumentation
#### Method 1
Generating image translations and horizontal reflections.
Do this by extracting random 224\*224 patches(and their horizontal reflections) from the 256\*256 images.

#### Method 2
Alering the intensities of the RGB channels in training images.
Do this by performing PCA on the set of RGB pixel values throughout the ImageNet training set. To each training image, add multiples of the found rincipal components, which magnitudes proportional to the corresponding eigenvalues times  a random variable drawn from a Gaussian with mean zero and standard deviation 0.1. Therefore to each RGB imge pixel $I\_{xy}=[I\_{xy}^{R}, I\_{xy}^{G}, I\_{xy}^{B}]^{T}$, add the following quantity:

$$[p1, p2, p3][\alpha_1\lambda_1, \alpha_2\lambda_2, \alpha_3\lambda_3]^T$$
where $p_i$and$\lambda_i$are ith eigenvector and eigenvalue of the 3\*3 covariance matrix of RGB pixel values, respectively.
This data argumentation method can reduce the accuracy by 1 pecent.

### Dropout

Dropout is an efficient way to combine different models. It forced to learn more robust features that are useful in conjunction with many different random subsets of the other neurons. 

At test time, they use all the neurons but multiply their outputs by 0.5, which is a resonable approximation to taking the geometric mean of the predictive distributions produced by the exponentially-many dropout networks.

The AlexNet use dropout in the first two fully-connected layers to reduce overfitting, however, this roughly doubles the number of iterations required to converge.

## Details of learning
SGD decent with a batch size of 128, momentum of 0.9 and weight decay of 0.0005.
Small amount of weight decay is import for the model to learn!

Initialize the weights in each layer from a zero-mean Gaussian distribution with standard deviation 0.01. 

Initialize the neuron biases in the second, fourth, dand fifth convolutional layers, as well as in the fully-connected hidden layers, with the constant 1. This initializations accelerates the early stages of learning by providing ReLUs with positive inputs, initialize the neuron biases in the remaning layers with the constant 0.

The heuristic AlextNet followed was to divide the learning rate by 10 when the validation error rate stopped improving with the current learning rate. The learning rate was initialized at 0.01 and reduced three times prior to termination.

Train the network for roughly 90 cycles though the training set of 1.2 million images, which took five to six days on two NVIDIA GTX 580 3GB GPUs.


## Discussion
Depth is important: removing any single convolutional layer will cause accuracy loss.


## 网络解析
```
net: ------------- AlexNet -------------
op: Conv (data conv1_w conv1_b)->(conv1) stride:4 pad:0 kernel:11 order:NCHW
op: Relu (conv1)->(conv1)
op: LRN (conv1)->(norm1 _norm1_scale) size:5 alpha:0.0001 beta:0.75 bias:1 order:NCHW
op: MaxPool (norm1)->(pool1) stride:2 pad:0 kernel:3 order:NCHW legacy_pad:3
op: Conv (pool1 conv2_w conv2_b)->(conv2) stride:1 pad:2 kernel:5 order:NCHW group:2
op: Relu (conv2)->(conv2)
op: LRN (conv2)->(norm2 _norm2_scale) size:5 alpha:0.0001 beta:0.75 bias:1 order:NCHW
op: MaxPool (norm2)->(pool2) stride:2 pad:0 kernel:3 order:NCHW legacy_pad:3
op: Conv (pool2 conv3_w conv3_b)->(conv3) stride:1 pad:1 kernel:3 order:NCHW
op: Relu (conv3)->(conv3)
op: Conv (conv3 conv4_w conv4_b)->(conv4) stride:1 pad:1 kernel:3 order:NCHW group:2
op: Relu (conv4)->(conv4)
op: Conv (conv4 conv5_w conv5_b)->(conv5) stride:1 pad:1 kernel:3 order:NCHW group:2
op: Relu (conv5)->(conv5)
op: MaxPool (conv5)->(pool5) stride:2 pad:0 kernel:3 order:NCHW legacy_pad:3
op: FC (pool5 fc6_w fc6_b)->(fc6)
op: Relu (fc6)->(fc6)
op: Dropout (fc6)->(fc6 _fc6_mask) ratio:0.5 is_test:1
op: FC (fc6 fc7_w fc7_b)->(fc7)
op: Relu (fc7)->(fc7)
op: Dropout (fc7)->(fc7 _fc7_mask) ratio:0.5 is_test:1
op: FC (fc7 fc8_w fc8_b)->(fc8)
op: Softmax (fc8)->(prob)
```
下面是该网络的结构图。注意前半部分是网络的前向迭代部分，后半部分是网络的后向迭代部分，即更新权重的部分。

![AlexNet Archetecture](/images/blogs/alexnet/archetecture.png)